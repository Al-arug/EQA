\documentclass[11pt, a4paper]{article}

\usepackage{mlt-thesis-2015}

% With Xetex/Luatex this shouldn't be used
%\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{float}
\usepackage{hyperref}
\usepackage{subfig}



\title{Embodied question answering in\\ robotic environment}
\subtitle{Automatic generation of a synthetic question-answer data-set }
% NI 21-09-27: author line
\author{Ali Aruqi}

% NI 21-09-27: I think we need a more specific subtitle, at the moment this describes what the original paper has done (e.g. created a synthetic datasets of questions and answers), but in current study you have extended their dataset and generated a different type of questions. Also, you have discovered and analysed shortcomings of the EQA dataset. 'Embodied Question Answering in a 3D World: Towards More Realistic and Diverse Questions and Answers'? Just an idea.

\begin{document}

%% ============================================================================
%% Title page
%% ============================================================================
\begin{titlepage}

\maketitle

\vfill

% NI 21-09-27: \iffalse (in the beginning) and \fi (in the end) can be used to comment out big chunks of text; more convenient if you don't want to spend time on placing % signs for each line
\iffalse
\begingroup
\renewcommand*{\arraystretch}{1.2}
\begin{tabular}{l@{\hskip 20mm}l}
\hline
Master's Thesis: & 30 credits \\
Programme: & Master’s Programme in Language Technology\\
Level: & Advanced level \\
Semester and year: & Spring, 2020\\
Supervisor & Simon Dobnik, Nikolai llinykh\\
Examiner & Staffan Larsson\\
Report number & number will be provided by the administrators \\
Keywords & Artificial Intelligence,  Natural Language Processing,\\ Computer Vision, \ \ Visual Question Answering, Synthetic Data-sets
\end{tabular}
\endgroup
\fi

% NI 21-09-27: There was some weird formatting: text in the second column were always outside of the page itself. I have re-formatted things. Very similar to what was before, but a bit different. Use tabular* to fit text based on the linewidth. Use p{} in tabular environment to fit text in the table instead of l/r/c.
\begingroup
\renewcommand*{\arraystretch}{1.2}
\begin{tabular*}{\textwidth}{l@{\hskip 20mm}p{0.6\linewidth}l}
\hline
Master's Thesis: & 30 credits\\
Programme: & Master’s Programme in Language Technology\\
Level: & Advanced Level \\
Semester and year: & Autumn, 2021\\
Supervisor & Simon Dobnik, Nikolai llinykh\\
Examiner & Staffan Larsson\\
Report number & (number will be provided by the administrators)\\
% NI 21-09-27: Star the keyword list with more specific phrases, which apply to your thesis. Then, gradually make it more general. Not sure how many keywords are required, but 4-5 is a good number, in general.
Keywords & Embodied Question Answering, Question Generation, Spatial Relations, Synthetic Datasets, Multi-Modality
\end{tabular*}
\endgroup


\thispagestyle{empty}
\end{titlepage}

%% ============================================================================
%% Abstract
%% ============================================================================
\newpage
\singlespacing
\section*{Abstract}
% NI 21-09-27: copying some of my comments from the previous review, I think they are still valid.
% What about structure like that: start with the general description of the task, specify what is the problem, describe your proposal to solve this problem, conclude what are your results and how your solution has helped to improve the state of this dataset.
% (draft, feel free to edit) Embodied Question Answering is the task of answering questions about virtual environment. In this task, the agent is spawned in a 3D world, where it is required to navigate to the object that the question asks about. The agent is trained on simple questions which do not require any complex spatial reasoning, typical for navigation in a real world (e.g., colour questions). Therefore, one problem with this dataset is the lack of questions which require complex spatial grounding  to be answered. In this work, we extend the original EQA dataset with more complex questions, which ask about size and position of the objects. Specifically, we…(details about questions). Our results show that…(concrete results). This work provides initial investigation of...(describe why this work is important and where it can be applied/how it can be extended).

Our work extends a dataset for Embodied-Question-Answering. Embodied question answering is the task of asking a robot a question about objects in a 3D environment, where the agent is expected to navigate the environment and find the entities in question and answer. The answer system consists of navigation and VQA components.  Each question in the dataset is an executable function that could be run in the environment to yield an answer.  The published dataset for EQA is EQA-V1, and it is a limited dataset that includes only two types of questions, color and location questions. We use the navigational data, required for training the system, from EQA-V1 and generate new questions of two more types, size and spatial questions. Our data extension is intended to better train the system and enhance its ability in performing the task. 

\thispagestyle{empty}

%% ============================================================================
%% Preface
%% ============================================================================
\newpage
\section*{Preface}

Acknowledgements, etc.

\thispagestyle{empty}

%% ============================================================================
%% Contents
%% ============================================================================
\newpage

\begin{spacing}{0.0}
\tableofcontents
\end{spacing}

\thispagestyle{empty}

%% ============================================================================
%% Introduction
%% ============================================================================
\newpage
\setcounter{page}{1}

% NI 21-09-27: General advice: write every sentence in the new line. This would significantly simplify tracking and editing for all three of us.

% NI 21-09-27: When citing, you do not have to wrap citations in (). This is done automatically. Also, there are different ways to cite when you simply mention the reference to support something that you are saying, or when you are describing what has been done by specific reference. Use \citep{} for the first case, \citet{} for the second case. I have formatted the first paragraph as an example.
% NI 21-09-27: Sometimes, a different style would work as well - \citep{} when mentioning references and \cite{} when embeddings them in text (instead of \citet{}).

\section{Introduction}
\label{sec:intro}

An intelligent robot must be able to understand and resolve references in its environment \citep{russell1995artificial}.
Our human ability to interact with our visual surroundings, manifested in language, stems from faculties such as perception and memory \citep{regier1996human}.
Perception, in particular, is central to our physical experience of the world \citep{barsalou1999perceptual}.
We conceive the physical world through perception, and we express our %conceptualization - NI 21-09-27: use British spelling consistently
conceptualisation of the perceptual experience in words \citep{lakoff2008metaphors}.
% NI 21-09-27: I use the next sentence as an example of using \citet{}. You can see that the reference now looks like it is a part of the text. You would normally use \citet{} when you want to embed your reference in the text or maybe when you want to give details about someone's work.
%Therefore, the exhibition of intelligent %behavior
%behaviour is necessitated by having a notion of meaning that associates 'words' with the visual/physical world \citep{nilsson2007physical}.
Therefore, as \citet{nilsson2007physical} argues, the exhibition of intelligent behaviour is necessitated by having a notion of meaning that associates 'words' with the visual/physical world.

\subsection{meaning}

% NI 21-09-27: Write ``chair'', not ''chair''. Use ``[word]'', not `[word]' or '[word]' or any other variation, stick to ``[word]''. This is sort of general rule that everyone follows.

% NI 21-09-27: You often want to format things differently if they are used as something OTHER than plain text. For example, you should reformat (c, h, a, i, r), this can be done in many ways - the main thing is that your formatting is consistent throughout the whole manuscript. For example:  {[ \texttt{c}, \texttt{h}, \texttt{a}, \texttt{i}, \texttt{r} ]}. If you put this in text, you will see that this looks a bit different. There is no right format for each case, people normally use something they have seen in other papers. Again, the main point: be consistent and use the same formatting every time you talk about letters in the word (as in this case).

The meaning of words is not a mere psychological phenomenon. Concrete nouns, for example, have references in the physical world, with physical properties indicated by their meaning. The meaning of a word is, thus, not only bound up with linguistic characters and mental notions but also with some physical representation in the world. For example, the word ``chair'' is represented by its token-characters (c, h, a, i, r), contains a perceptual symbolism(mental understanding of the chair's attributes and functions), and refers to an entity with physical features in the world(\cite{mooney}).

% NI 21-09-27: a good place to refer to the paper by Bender and Koller: https://aclanthology.org/2020.acl-main.463/ and all other research that shows that meaning of words is grounded in reality. You can emphasise the need for grounded agents, which is a hot topic these days (e.g., big language models do not capture real meaning of words, because they are not grounded). This need also strengthens and motivates your current work - you are working on the multi-modal system that answers questions (some sort of interaction setting, not just static captioning).

In this triangular definition of meaning, 'vision' has an integral part of the meaning in which it represents the physical world to language. To recognize a chair, one should, for example, identify the existence of legs, seats, their sizes, and geometric shape in a visual scene. % NI 21-09-27: I see what you want to say, but we also often do not need to see object as a whole. I can see legs of the table to predict that the whole object is the table. This happens because I conceptually know what are the parts of a prototypical table since I have encountered tables before and learned how they look. Visual recognition does not form only from perception, our experiences and knowledge also play a role. This is related to my next point about rotten food: if I know that bad fruit colour indicates that the food is rotten (because I have seen and tasted it before in this state), I can already predict that the food is rotten by merely looking at it.
These properties of the physical reference of a chair can be clearly represented in a visual form. Therefore visual recognition is part of the conceptualization process that forms the perceptual symbol of an entity.(\cite{barsalou1999perceptual})

Perceptual information, however, is more than just visual information. The properties of an object include other sensory information such as the smell, taste, and texture of an object. For example, the meaning of rotten food could be more understood if the food is tasted.
% NI 21-09-27: not necessarily, the food might also visually look rotten, this would be enough to infer that it is not fresh.

The formation of a symbolic representation (meaning) requires more than the recognition of perceptual information. The construct of a meaning (symbol) could only be formulated by the existence of knowledge about the relations of the attributes that form an entity.(\cite{barsalou1999perceptual}) refers to the process of forming a symbol as 'componential' or schematic, meaning that a  notion of meaning is a scheme that is conceptualized or constructed. These schemes can either be logically constructed in our mind in terms of holding valid truthiness about the world, or they can be based on incoherent conceptualization forming a false knowledge. 

The full meaning is the perceptual representation and the knowledge about it. The meaning of rotten apple is fully understood when we construct knowledge of the negative aspects of eating it. For example, rotten apples have red-brown colors, and the stomachache that results by eating them leads us to form the belief that red-brown apples are different from all-red apples, not only by color but also other health/taste attributes, so we classify red-brown apples in a different category called "rotten apples." The knowledge about health implications and the attributes such as the colors and smell of a rotten apple help us categorizing the rotten apple in the category of rotten food. (\cite{lakoff2008metaphors}) explains that this attributive characterization can be expressed, for example, in the way we do prototyping and categorization of entities.

% NI 21-09-27: the story above is scattered a bit. Start with saying which conditions are required to capture meaning. Then, go into details of each of these conditions. You can go from saying that meaning is not merely in texts, but also around us. We use words to refer to the world around and communicate it. Frege's example of Morning Start/Evening Star - two different phrases are used to refer to the same object. Then, you would move to perception (vision) and knowledge (experiences, etc.). As I can see, you want to show that meaning is not only in texts, but also in vision, taste, sensor information. Try to go from text to multi-modality here.


\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{images/symbolsyststem.png}
\caption{\cite{roy2005semiotic}}
\label{fig:roy}
\end{figure}.
% NI 21-09-27: every figure should be accompanied by a caption that describes what this figure shows. Citation or simple phrase is not enough.

% NI 21-09-27: below, you are moving to meaning in interaction with the world, right? A smoother connection with previous text is needed.
% NI 21-09-27: when referring to figures, write Figure~\ref{fig:roy}

Interactions in the semantic world have an exchangeable nature. The interaction allows us to form meaning, and the formed meaning shapes our language and actions. In Figure~\ref{fig:roy} we see that the outcomes of our interaction in the world includes not only linguistic implications but also affects the actions(\cite{roy2005semiotic}). 'schemes about the world' are the beliefs we make from the interactions. For example, our negative experience with the red-brown apple made us form the  "belief" that rotten apples are bad. The knowledge that "rotten apples are bad" influences our future actions- makes us not eat the apples with attributes of "rotten." 

The process of comprehending meaning through associating attributes with each other to form a belief or draw a conclusion denotes the notion of reasoning. The process of classifying the apple as rotten includes multiple abstractions. We might first identity the apple by its general shape structure, then recognize, from previous experiences, that apples in red-brown color are not like all-red apples, then conclude that the apple is rotten. Reasoning is the ability to take the logical steps to conclude.  
 


\subsection{Grounding meaning}

The approaches to ground meaning(form meaning) vary depending on the aspect of meaning that each approach focuses on. The different methods we review below-approaching meaning as a mental notion, a map of connected knowledge nodes,  vision and language representation, or the combination of different aspects of meaning representation. 

Word-meaning in a Vector Semantic Space (VSM) represents a mental aspect of meaning notion(\cite{Turney_2010})). Space can be understood by imagining our minds as a space that we allocate meaning representation in them. In VSM, the mind (represented as neural language model) is an artificial space where word meanings are allocated at different distances from each other depending on their categorization, such as a rotten apple is closer to fresh apples than to a chair. 

There are multiple hypotheses to representing word-meaning in a Vector Semantic Space(VSM). Distributional hypothesis is a popular example of word representation in VSM. The premise of this approach is that language is compositional, and word-meaning can be defined by its context--The meaning of a word is represented by the word and the words surrounding it \cite{Turney_2010}. The formulated word meaning representation is known as "word-embeddings" \cite{mikolov2013distributed}. Using language to define language has proven promising in inferential tasks such as inferring that "university" and "student" are close to each other given their common context of "education." 

% NI 21-09-27: I am not exactly sure if you really need the paragraphs above. In the context of this thesis, grounded meaning is about multi-modal meaning. Word embeddings are not multi-modal and vector semantic space does not really fit here. This is about general meaning representations for computation, while you are focusing on several modalities for meaning. Maybe you can re-phrase the stuff above and just say that meaning can be uni-modal (mention word embeddings etc)?
 
% NI 21-09-27: The focus in this subsection should be on multi-modal grounded meaning. You need to add more discussion here. https://aclanthology.org/P12-1015.pdf - this paper has some good references in the introduction about multi-modal meaning. Another good reference: https://aclanthology.org/N10-1011.pdf.
Representing meaning with a combined linguistic and visual representation is the second example of an extensively researched field. Early research in combining vision and language used probabilistic learning by aiming at drawing an alignment between sentences, phrases, and words with the corresponding perceptual representations.\cite{790410}. An approach to probabilistic learning estimates the probability of a grammatical entity(text) being related to a perceptual representation \cite{6751319}. A second probabilistic method is classifying each word in a sentence through the probability distribution of words over a perceptual representation. In \cite{matuszek2012joint} \cite{larsson2015formal} we see examples of connecting entities of formal semantics(First Order Logic) with perception. 

There are examples of research that attempt to incorporate knowledge graphs in the representation of meaning. Examples, \cite{zhu2015building}, \cite{zhu2014reasoning}. 
%Despite the variety of methods in forming meaning, the comprehension of a full meaning cannot be formed in exclusion of any of its aspects. Even abstract concepts can sometimes have a physical/visual representation such as Art. Art works cannot be understood if we do not perceptually perceive them and connect them to some mental notion of knowledge. In order to achieve such high cognition abilities in robots, scientist and researchers are continuously introducing finding new methods and approaches to improve the computer's abilities.  

%In  robots, a full comprehension of meaning is essential for its usability. 
%the ability to understand the visual aspect of meaning is essential for the robot's ability to interact and preform action. 

%A paragraph about neural language models for vision and language (intro for image captioning)





%\cite{dunning1993} introduced a well-known method for extracting
%collocations. Bilingual data can be used to train part-of-speech
%taggers \citep{das2011}. Another one: \citep{cortes2014}

%Testing Unicode: Göteborgs universitet

%\textit{Testing} \textbf{testing} \textsc{testing} some font series.

%Testing a formula:
%\[
%P(X) = \sum_{i=1}^N P(A_i) P(X|A_i)
%\]

%Testing a table:
%\begin{table}[htbp]
%\begin{center}
%\begin{tabular}{c|c}
%cell 1 & cell 2 \\
%\hline
%cell 3 & cell 4
%\end{tabular}
%\caption{This is a table.}
%\end{center}
%\end{table}



\newpage

\section{background}
\label{sec:background}
\input{background}

\newpage

\section{Methods and materials (proof reading required)}
\label{sec:methods}
\input{methods.tex}

\newpage

\section{Task One- Question Generation (Final review required)}
\label{sec:task1}
\input{task1}


\section{Task Two- Question Asking (Text to be added)}
\label{sec:task2}
\input{task2}


\addcontentsline{toc}{section}{References}
\bibliography{references}

\newpage
\section{Appendices}


\subsection{datasets} 

The 3D Scenes and the QA dataset mentioned in \cite{embodiedqa}, are called SUNCG(3D houses) and "EQA V1" (QA). The EQA V1 is a synthetic dataset generated automatically, and constructed based on the setting of the 3D houses in SUNCG. SUNCG is no longer available. \cite{embodiedqa} changed the SUNCG 3D setting to MatterPort 3D (MP3D). MatterPort 3D is a reconstruction of 3D houses in (SUNCG) scene dataset. The latter also implies that the inital "EQA V1" is not applicable for MP3D. 

The new QA dataset for Matterport 3D is available but not the code that generated it. The EQA-mp3d v1 is also a synthetic dataset generated automatically and can be found at this footnote reference \footnote{https://github.com/facebookresearch/habitat-lab}. For generating  questions for SUNCG, a code published at this reference\footnote{https://github.com/facebookresearch/EmbodiedQA}. However, there is no code for generating QA for MP3D. 
 
A few of the differences between the question dataset for SUNCG (EQA-SUNCG) and MP3D(EQA-MP3d) are mentioned in \cite{eqa_matterport}. However, not in all the information in  \cite{eqa_matterport} seems to match with EQA-MP3D that we have. In  \cite{eqa_matterport} page(4) it's stated that the number of scene used from MP3D is 76. The dataset we downloaded from "facebookai/habitat" repo on github uses a total 67 scene of 90 scenes available in MatterPort3D. 57 of the 67 scenes are used for questions in the train-set and 10 in the the enviroment. Note that the latter implies that the robot is tested on different scenes from the scenes it has been trained in. 


\subsubsection{List of textual references with number of answer choices}

. 
\begin{figure}[H]
    \centering
    \subfloat[]{{\includegraphics[width=6cm]{images/plot1.png}}}%
    %\qquad
    \subfloat[]{{\includegraphics[width=6cm] {images/plot2.png}}}%
    \caption{Each row is a textual reference. The number of answer choices for each textual reference is represented by the colorful blocks. One block = 1 answer choice and so on. The colors of the bars are not representative of the named color answer so they should be disragarded}
\end{figure}.



\subsection{Habitat-lab(EQA evaluation)}

\begin{figure}[H]
\centering
\includegraphics[scale=0.43]{images/configProcess.png}
\caption{Example of Habitat lab processing the configurations to implement validation for the VQA model }
\label{fig:configs}
\end{figure}

Figure \ref{fig:configs} resembles a map of the code structure when the habitat lab module is initiated to preform validation task for the VQA. Each task has its own configurations and in this example the task is 'VQA evaluation'. As seen in the figure \ref{fig:configs}, the module takes hierarchical steps in which each step is executed in accordance to the configuration of the given task. In the most down box of the structure we see parts of the commands directed for the simulator, such as insinuating an environment and sensors in the agent. Other commands include registering a data-set which takes part in lab module. 


The configurations are processed into commands in Habitat-lab before being passed to the simulator. Habitat lab is the second core component of the system. In addition to giving commands to the simulator, the Habitat Lab module acts as a pipeline that prepares the data-set of the corresponding task. The habitat-lab module,in other words, is the coordinator that informs the simulator of the required setting, and the data loader and processor that prepares the data for either training or testing. 



y in science and
engineering (from aerospace to zoology). In the context of
embodied AI, simulators help overcome the aforementioned
challenges – they can run orders of magnitude faster than
real-time and can be parallelized over a cluster; training
in simulation is safe, cheap, and enables fair comparison
and benchmarking of progress in a concerted communitywide effort. Once a promising approach has been developed
and tested in simulation, it can be transferred to physical
platforms that operate in the real world

. The name 'Habitat' is derived from  the notion of learning within and from an environment. Imitating our natural habitat, the Habitat platform facilitates spawning an agent in a simulated environments with the possibility of teaching the robot to preform different tasks. 

The perquisites needed to test or train an agent for a certain task in a given environment, are facilitated by a core component called  Habitat Simulator. Habitat Simulator is responsible for simulating an  environment and insinuating a robot in it. The simulator acts depending on the configurations given to it. 



\end{document}


